---
tags:
---
Anyone can devise new/understand old deep learning architectures as long as:
1. understand and therefore have faith in the power of automatic backward gradient propagation
	- make sure the operations are differentiable
	- so long they are all differentiable, don't worry it won't learn 
2. think of/inspire by something that appropriately represent data (input, output, inner representations)
	- as long as there is representation, your model can be used on any (combination of) data modality (text, image, program, video, audio)
3. be aware of the bells and whistles that control the variance and regulates the model (norm, residual, dropout)
4. think of inner layers as transformations of data representation. 